{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import h5py\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Data Loading</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading and combining the two csv files\n",
    "\n",
    "# HDF5 file with exam_id and tracings\n",
    "filename = \"./data/exams_part0.hdf5\"\n",
    "\n",
    "# CSV file with exam_id, patient_Id and label\n",
    "df_sample1 = pd.read_csv(\"./data/code15_chagas_labels.csv\")\n",
    "\n",
    "# main CSV file\n",
    "df_sample2 = pd.read_csv(\"./data/exams.csv\")\n",
    "\n",
    "lista = ['exam_id','patient_id']\n",
    "\n",
    "# csv file with exam_id, patient_id and label\n",
    "df_master = pd.merge(df_sample2, df_sample1, on=lista, how='left')\n",
    "\n",
    "# saving it as  a file\n",
    "df_master.to_csv('data/exams_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Butterworth bandpass filter, shortening functions and resampling function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "# Bandpass filter\n",
    "def butter_bandpass_filter(data,lowcut, highcut, fs,order):\n",
    "\n",
    "    nyquist_freq = 0.5 * fs\n",
    "    low = lowcut / nyquist_freq\n",
    "    high = highcut / nyquist_freq\n",
    "\n",
    "    b,a = scipy.signal.butter(order, [low, high], btype='band')\n",
    "    y = scipy.signal.filtfilt(b, a, data,axis=0)\n",
    "\n",
    "    return y\n",
    "\n",
    "def shorten_Code(signal):\n",
    "    \n",
    "    if signal.shape[0] > 4000:\n",
    "        # riconverto il segnale in un tensore, utilizzo la copy per evitare errori con stripe negativa\n",
    "        signal = torch.tensor(signal.copy())\n",
    "        # taglio il segnale a 10 secondi che corrispondono a 4000 campioni a 400 Hz\n",
    "        signal = torch.narrow(signal, 0, 0, 4000)\n",
    "    return signal\n",
    "\n",
    "def shorten_Sami(signal):\n",
    "    \n",
    "    if signal.shape[0] == 4096:\n",
    "        \n",
    "        start = 48\n",
    "        end = 4048\n",
    "        sliced_signal = signal[start:end,:]\n",
    "    return sliced_signal\n",
    "\n",
    "def resample(signal, fs, fs_new):\n",
    "    try:\n",
    "        resampled_signal = np.zeros((4000, signals.shape[1]))\n",
    "        for i in range(signals.shape[1]):\n",
    "\n",
    "            res_sig,_ = resample_sig(signals[:,i], fs, fs_new)\n",
    "            resampled_signal[:,i] = res_sig\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return resampled_signal\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Code15% data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving important information in a dictionary\n",
    "\n",
    "import h5py\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "# reading file HDF5 separing the two datasets\n",
    "df_master = pd.read_csv('data/exams_labels.csv')\n",
    "# pupulating the dictionary exam_ids_and_signals\n",
    "exam_ids_to_chagas = dict()\n",
    "\n",
    "# array with tuples\n",
    "tuples= []\n",
    "\n",
    "with open(\"data/merged.csv\", newline=\"\\n\") as csvfile:\n",
    "    \n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        \n",
    "        exam_id = int(row['exam_id'])\n",
    "        boolean = bool(row['chagas'])\n",
    "        exam_ids_to_chagas[exam_id] = boolean\n",
    "\n",
    "\"\"\"\n",
    "    Reading the HDF5 files. Each signal is filtered\n",
    "\"\"\"\n",
    "\n",
    "files = [\"./data/exams_part\"+str(i)+\".hdf5\" for i in range(0,18)]\n",
    "\n",
    "for filename in files:\n",
    "    # if the file doesn not exist, skip it\n",
    "    if (os.path.exists(filename) == False):\n",
    "        continue\n",
    "\n",
    "    print(\"Reading file: \", filename)\n",
    "\n",
    "    with h5py.File(filename, \"r\") as ecgs:\n",
    "\n",
    "        exam_ids = list(ecgs['exam_id'])\n",
    "        num_exams = len(exam_ids)\n",
    "        for i in range(num_exams):\n",
    "            \n",
    "            exam_id = exam_ids[i]\n",
    "            if exam_id not in exam_ids_to_chagas:\n",
    "                \n",
    "                continue\n",
    "            else:\n",
    "                \n",
    "                filtered_signal = butter_bandpass_filter(ecgs['tracings'][i], 0.5, 40, 400, 3)\n",
    "                filtered_signal = shorten_Code(filtered_signal)\n",
    "                tuples.append((filtered_signal, exam_ids_to_chagas[exam_id]))\n",
    "\n",
    "print(\"Number of Exams:\",len(exam_ids_to_chagas))\n",
    "print(\"Number of Tuples: \", len(tuples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Sami-Trop</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every patient has Chagas\n",
    "import csv\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Reading the CSV file to get the exam_ids of the SAMI dataset.\n",
    "\"\"\"\n",
    "sami_exam_ids = []\n",
    "with open(\"data/examsSAMI.csv\", newline=\"\\n\") as csvfile:\n",
    "    \n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        \n",
    "        exam_id = int(row['exam_id'])\n",
    "        sami_exam_ids.append(exam_id)\n",
    "\n",
    "\"\"\"\n",
    "    Reading the HDF5 file. Each signal is filtered and then sliced to 4000 samples (10 seconds).\n",
    "\"\"\"\n",
    "with h5py.File(\"data/examsSAMI.hdf5\", \"r\") as ecgs:\n",
    "    \n",
    "    num_exams = len(sami_exam_ids)\n",
    "    for i in range(num_exams):\n",
    "        \n",
    "        signal = butter_bandpass_filter(ecgs['tracings'][i], 0.5, 40, 400, 3)\n",
    "        signal = shorten_Sami(signal)\n",
    "        tuples.append((signal, True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>PTB-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import wfdb\n",
    "from wfdb.processing import resample_sig\n",
    "ptb_ecg_ids = [] \n",
    "ptb_ecg_and_labels = dict()\n",
    "\n",
    "main_dir = \"data/records500/\"\n",
    "\n",
    "# reading the csv file in order to get the exam_id and adding the label in the dictionary\n",
    "with open(\"data/ptbxl_database.csv\", newline=\"\\n\") as csvfile:\n",
    "    \n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        \n",
    "        ecg_id = row['ecg_id']\n",
    "        if ecg_id not in ptb_ecg_ids:\n",
    "            \n",
    "            ptb_ecg_ids.append(ecg_id)\n",
    "        if ecg_id not in ptb_ecg_and_labels:\n",
    "            \n",
    "            ptb_ecg_and_labels[ecg_id] = False\n",
    "\n",
    "\"\"\"\n",
    "    Reading the PTB-XL dataset. Each signal is filtered and then resampled to 400 Hz.\n",
    "\"\"\"\n",
    "for subfolder in sorted(os.listdir(main_dir)):\n",
    "    \n",
    "    subfolder_path = os.path.join(main_dir, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "\n",
    "        dat_files = [f for f in os.listdir(subfolder_path) if f.endswith('.dat')]\n",
    "        for dat_file in dat_files:\n",
    "\n",
    "            record_name = os.path.join(subfolder_path, dat_file[:-4]) \n",
    "            try:\n",
    "                # Read the signal and header\n",
    "                record = wfdb.rdsamp(record_name)\n",
    "                # Extract signal and metadata\n",
    "                signals, fields = record\n",
    "                # Apply bandpass filter, resample and shorten the signal\n",
    "                signals = butter_bandpass_filter(signals, 0.5, 40, 500, 3)\n",
    "                signals = resample(signals, 500, 400)\n",
    "                signals = shorten_Code(signals)\n",
    "                tuples.append((signals, False))\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f\"Error reading {dat_file}: {e}\")\n",
    "\n",
    "print(\"Number of ECGS: \", len(tuples)) #83430"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from custom_dataset import FinalDataset\n",
    "\n",
    "dataset = FinalDataset(tuples)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
